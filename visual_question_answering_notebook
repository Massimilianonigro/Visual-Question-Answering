{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Rita Levi Mortal Kombat.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jM2ukdDX4tDY",
        "vZAy5DBZsFo_",
        "pgM71MuhsJ1L",
        "wBijykeXsRzu",
        "SNKb_-pCAKRP",
        "nVJDV1oXsaMx",
        "Q1mJoyg6toGn",
        "M_cyC88HC1bD",
        "jJA4IavpsvAN",
        "GyitCZGrtEgT",
        "TBNqH10csgGW",
        "BH0iJMnssjCp",
        "Pl8uFqu1sk99",
        "tqvXMqbqsouR"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Massimilianonigro/Visual-Question-Answering/blob/main/visual_question_answering_notebook\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM2ukdDX4tDY"
      },
      "source": [
        "#VISUAL QUESTION ANSWERING\n",
        "We started from a base model which comprised a convolutional net in parallel with a recurrent Net. The convolutional section dealt with images while the convolutional part delt with questions (natural language).  \n",
        "Approaches:\n",
        "- Base ensemble model: we trained 3 different networks for each question category:\n",
        "  - yes/no\n",
        "  - counting\n",
        "  - other\n",
        "  on top of this we trained a classifier which would recognise the class to which each question belonged. The classifier was a recurrent network with Bidirectional GRU.\n",
        "  Accuracy 0.034 (probably due to an error at the prediction phase)\n",
        "- Transfer Model: we used NasNetMobile (Large) as the convolutional network and left GRUs on the recurrent network. We left the whole transfer model frozen and then we ran another training round with the model unfrozen and a low learning rate (fine tuning)\n",
        "Accuracy 0.363 (not fine tuned)\n",
        "Accuracy 0.365 (fine tuned)\n",
        "- Transfer Model V2: on top of the transfer model we also pre-loaded glove weights on the embedding layer and marked it as non trainable. \n",
        "Accuracy 0.25 (not fine tuned)\n",
        "- Transfer Model V2 (ensemble): we followed the same procedure of the base ensemble model but we used the transfer model v2 as a base, moreover we tweaked for each of the 3 question models regularization in order to better fit the different problems\n",
        "Accuracy 0.12\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndo0FOClD285",
        "outputId": "b6191738-6541-4f81-94c7-3663c1951207"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/VQA_Dataset.zip\" .\n",
        "!mkdir Results\n",
        "!unzip -q VQA_Dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NiGfRU0fg-h",
        "outputId": "11fa9d8d-ca26-4747-8050-89df513ad546"
      },
      "source": [
        "!pip install focal-loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting focal-loss\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/96/babb0f40b2046a45aa2263773d915a34f02d4fb6bae91a505ce2db8ab0b2/focal_loss-0.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.6/dist-packages (from focal-loss) (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (0.3.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.32.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.12)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=2.2->focal-loss) (51.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (4.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Installing collected packages: focal-loss\n",
            "Successfully installed focal-loss-0.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnWyMgFNfgud"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyhbGNkAHGyd"
      },
      "source": [
        "# Cell output set up for Jupyter\n",
        "from pathlib import Path\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQlGKPlKHLUi"
      },
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import random \n",
        "import math\n",
        "\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAy5DBZsFo_"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVVez4_yHVCg"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, Union, Optional, Callable\n",
        "\n",
        "@dataclass\n",
        "class Config: \n",
        "  max_length: int = None\n",
        "  batch_size: int = 64\n",
        "  split: float = 0.8\n",
        "  dataset_name: Path = Path(\"VQA_Dataset\")\n",
        "  augmentation: Dict[str, Union[bool,int,Optional[Callable],str]] = None\n",
        "  # 2 in case of yes_or_no, 6 in case of counting, 50 in case of other, 58 in case of all\n",
        "  num_classes: int = 58\n",
        "  img_w: int = 350\n",
        "  img_h: int = 200\n",
        "  wtoi: int = None\n",
        "  #Questions can be 'all','yes_or_no','counting','other'\n",
        "  questions: str = 'all'\n",
        "  tokeinizer = None\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umy9jUAoSJuE"
      },
      "source": [
        "labels_dict = {\n",
        "        '0': 0,\n",
        "        '1': 1,\n",
        "        '2': 2,\n",
        "        '3': 3,\n",
        "        '4': 4,\n",
        "        '5': 5,\n",
        "        'apple': 6,\n",
        "        'baseball': 7,\n",
        "        'bench': 8,\n",
        "        'bike': 9,\n",
        "        'bird': 10,\n",
        "        'black': 11,\n",
        "        'blanket': 12,\n",
        "        'blue': 13,\n",
        "        'bone': 14,\n",
        "        'book': 15,\n",
        "        'boy': 16,\n",
        "        'brown': 17,\n",
        "        'cat': 18,\n",
        "        'chair': 19,\n",
        "        'couch': 20,\n",
        "        'dog': 21,\n",
        "        'floor': 22,\n",
        "        'food': 23,\n",
        "        'football': 24,\n",
        "        'girl': 25,\n",
        "        'grass': 26,\n",
        "        'gray': 27,\n",
        "        'green': 28,\n",
        "        'left': 29,\n",
        "        'log': 30,\n",
        "        'man': 31,\n",
        "        'monkey bars': 32,\n",
        "        'no': 33,\n",
        "        'nothing': 34,\n",
        "        'orange': 35,\n",
        "        'pie': 36,\n",
        "        'plant': 37,\n",
        "        'playing': 38,\n",
        "        'red': 39,\n",
        "        'right': 40,\n",
        "        'rug': 41,\n",
        "        'sandbox': 42,\n",
        "        'sitting': 43,\n",
        "        'sleeping': 44,\n",
        "        'soccer': 45,\n",
        "        'squirrel': 46,\n",
        "        'standing': 47,\n",
        "        'stool': 48,\n",
        "        'sunny': 49,\n",
        "        'table': 50,\n",
        "        'tree': 51,\n",
        "        'watermelon': 52,\n",
        "        'white': 53,\n",
        "        'wine': 54,\n",
        "        'woman': 55,\n",
        "        'yellow': 56,\n",
        "        'yes': 57\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFyOXbZayv3R",
        "outputId": "2e34332e-f407-449d-db65-b48d87bd1671"
      },
      "source": [
        "if config.questions == 'yes_or_no':\n",
        "  for key in set(labels_dict.keys()).difference({'yes','no'}):\n",
        "    del labels_dict[key]\n",
        "elif config.questions == 'counting':\n",
        "  for key in set(labels_dict.keys()).difference({'0','1','2','3','4','5'}):\n",
        "    del labels_dict[key]\n",
        "elif config.questions != 'all':\n",
        "  for key in {'yes','no','0','1','2','3','4','5'}:\n",
        "    del labels_dict[key]\n",
        "counter = 0\n",
        "for k in labels_dict.keys():\n",
        "  labels_dict[k] = counter\n",
        "  counter += 1 \n",
        "labels_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 0,\n",
              " '1': 1,\n",
              " '2': 2,\n",
              " '3': 3,\n",
              " '4': 4,\n",
              " '5': 5,\n",
              " 'apple': 6,\n",
              " 'baseball': 7,\n",
              " 'bench': 8,\n",
              " 'bike': 9,\n",
              " 'bird': 10,\n",
              " 'black': 11,\n",
              " 'blanket': 12,\n",
              " 'blue': 13,\n",
              " 'bone': 14,\n",
              " 'book': 15,\n",
              " 'boy': 16,\n",
              " 'brown': 17,\n",
              " 'cat': 18,\n",
              " 'chair': 19,\n",
              " 'couch': 20,\n",
              " 'dog': 21,\n",
              " 'floor': 22,\n",
              " 'food': 23,\n",
              " 'football': 24,\n",
              " 'girl': 25,\n",
              " 'grass': 26,\n",
              " 'gray': 27,\n",
              " 'green': 28,\n",
              " 'left': 29,\n",
              " 'log': 30,\n",
              " 'man': 31,\n",
              " 'monkey bars': 32,\n",
              " 'no': 33,\n",
              " 'nothing': 34,\n",
              " 'orange': 35,\n",
              " 'pie': 36,\n",
              " 'plant': 37,\n",
              " 'playing': 38,\n",
              " 'red': 39,\n",
              " 'right': 40,\n",
              " 'rug': 41,\n",
              " 'sandbox': 42,\n",
              " 'sitting': 43,\n",
              " 'sleeping': 44,\n",
              " 'soccer': 45,\n",
              " 'squirrel': 46,\n",
              " 'standing': 47,\n",
              " 'stool': 48,\n",
              " 'sunny': 49,\n",
              " 'table': 50,\n",
              " 'tree': 51,\n",
              " 'watermelon': 52,\n",
              " 'white': 53,\n",
              " 'wine': 54,\n",
              " 'woman': 55,\n",
              " 'yellow': 56,\n",
              " 'yes': 57}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgM71MuhsJ1L"
      },
      "source": [
        "#Dataset Split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyG0SyGfgrHu"
      },
      "source": [
        "def question_split():\n",
        "  annotations = json.load(config.dataset_name.joinpath(\"train_questions_annotations.json\").open())\n",
        "  yes_or_no_annotations = {}\n",
        "  counting_annotations = {}\n",
        "  other_annotations = {}\n",
        "  class_occurrences = {}\n",
        "  for label in labels_dict:\n",
        "    class_occurrences[label] = 0\n",
        "  \n",
        "  for k,v in annotations.items():\n",
        "    class_occurrences[v['answer']] = class_occurrences[v['answer']] + 1 \n",
        "    if v['answer'] in {'yes','no'}:\n",
        "      dict_to_write = yes_or_no_annotations\n",
        "    elif v['answer'] in {'0','1','2','3','4','5'}:\n",
        "      dict_to_write = counting_annotations\n",
        "    else:\n",
        "      dict_to_write = other_annotations\n",
        "    dict_to_write[k] = v\n",
        "  json.dump(yes_or_no_annotations,config.dataset_name.joinpath(\"yes_or_no.json\").open(\"w+\"))\n",
        "  json.dump(counting_annotations,config.dataset_name.joinpath(\"counting.json\").open(\"w+\"))\n",
        "  json.dump(other_annotations,config.dataset_name.joinpath(\"other.json\").open(\"w+\"))\n",
        "  json.dump(class_occurrences,config.dataset_name.joinpath(\"class_occurrences.json\").open(\"w+\"))\n",
        "question_split()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIRp0IkIIFB8"
      },
      "source": [
        "\n",
        "def split(config : Config,to_split : str): \n",
        "  \n",
        "  train_validation_dict = json.load(config.dataset_name.joinpath(to_split).open())\n",
        "  train_validation_key_set = set(train_validation_dict.keys())\n",
        "  question_num = len(train_validation_key_set)\n",
        "  question_num_train = math.floor(config.split * question_num)\n",
        "  question_num_val = question_num - question_num_train\n",
        "  validation_dict = {}\n",
        "  for i in range(0,question_num_val):\n",
        "    to_add_index = random.randint(0,question_num - i -1)\n",
        "    to_add_value = list(train_validation_dict.values())[to_add_index]\n",
        "    to_add_key = list(train_validation_dict.keys())[to_add_index]\n",
        "    validation_dict[to_add_key] = to_add_value\n",
        "    train_validation_dict.pop(to_add_key)\n",
        "\n",
        "  train_dict = train_validation_dict\n",
        "  json.dump(train_dict,config.dataset_name.joinpath(\"train.json\").open(\"w+\"))\n",
        "  json.dump(validation_dict,config.dataset_name.joinpath(\"validation.json\").open(\"w+\"))\n",
        "\n",
        "path = \"train_questions_annotations.json\" if config.questions == 'all' else \"yes_or_no.json\" if config.questions == 'yes_or_no' else \"counting.json\" if config.questions == 'counting' else 'other.json'\n",
        "split(config,path)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBijykeXsRzu"
      },
      "source": [
        "#VQA Custom Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C02kxjSW_Fo"
      },
      "source": [
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import OrderedDict\n",
        "\n",
        "def encode_input(data_dict):\n",
        "  question_list = []\n",
        "  question_max = 0\n",
        "  for v in data_dict.values():\n",
        "    question = v['question'].strip()\n",
        "    question_words = len(question.translate(str.maketrans(\"\",\"\",string.punctuation)).split(' '))\n",
        "    question_max = question_words if question_words > question_max else question_max\n",
        "    question_list.append(question)\n",
        "    #question_list_eos.append(question + '<eos>')\n",
        "    #question_list_sos.append('<sos>' + question)\n",
        "    # Create Tokenizer to convert words to integers\n",
        "  tokenizer = Tokenizer(num_words= question_max)\n",
        "  config.tokenizer = tokenizer\n",
        "  tokenizer.fit_on_texts(question_list)\n",
        "  tokenized = tokenizer.texts_to_sequences(question_list)\n",
        "  config.wtoi = len(tokenizer.word_index)\n",
        "  config.max_length = 14\n",
        "  return pad_sequences(tokenized,maxlen=config.max_length)\n",
        "\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self,config : Config, which_subset : str, image_generator=None):\n",
        "    self.config = config\n",
        "    self.image_generator = image_generator\n",
        "    self.which_subset = which_subset\n",
        "    if self.which_subset == 'training':\n",
        "      self.data_dict = json.load(Path(config.dataset_name).joinpath(\"train.json\").open(),object_pairs_hook=OrderedDict)\n",
        "    elif self.which_subset == 'validation':\n",
        "      self.data_dict = json.load(Path(config.dataset_name).joinpath(\"validation.json\").open(),object_pairs_hook=OrderedDict)\n",
        "    else:\n",
        "      raise Exception(\"Unsupported which subset: \"+ str(which_subset))\n",
        "    #question_list_eos = []\n",
        "    #question_list_sos = []\n",
        "    self.encoder_inputs = encode_input(self.data_dict)\n",
        "  def __len__(self):\n",
        "    return len(self.data_dict)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item_key = list(self.data_dict.keys())[index]\n",
        "    item_value = self.data_dict[item_key]\n",
        "    item_image_id = item_value['image_id']\n",
        "    item_image = Image.open(Path(self.config.dataset_name).joinpath(\"Images\",item_image_id +  \".png\"))\n",
        "    item_image = item_image.convert(\"RGB\")\n",
        "    item_image = item_image.resize((config.img_w,config.img_h))\n",
        "    item_image_arr = np.array(item_image).transpose((1, 0, 2))\n",
        "    if self.which_subset == 'training' and self.image_generator is not None:\n",
        "      transform = self.image_generator.get_random_transform(item_image_arr.shape, seed=SEED)\n",
        "      item_image_arr = self.image_generator.apply_transform(item_image_arr, transform)\n",
        "    target = int(labels_dict[item_value['answer']])\n",
        "    return {'image':item_image_arr , 'question':self.encoder_inputs[index]} , target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfUAFvIvdEOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "0fdc3cb8-0972-44f8-8774-836bfc6c5b6b"
      },
      "source": [
        "config.augmentation = {\n",
        "  'rotation_range':20,\n",
        "  'width_shift_range':0.1,\n",
        "  'height_shift_range':0.1,\n",
        "  'zoom_range':0.1,\n",
        "  'horizontal_flip':True,\n",
        "  'fill_mode':\"nearest\",\n",
        "  'rescale':1./255,\n",
        "  'preprocessing_function': None\n",
        "}\n",
        "\n",
        "image_generator = ImageDataGenerator(**config.augmentation)\n",
        "\n",
        "train_dataset = CustomDataset(config,\"training\",image_generator=image_generator)\n",
        "val_dataset = CustomDataset(config,\"validation\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-983616f58719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mimage_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f95f2be5e170>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, which_subset, image_generator)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#question_list_eos = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#question_list_sos = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f95f2be5e170>\u001b[0m in \u001b[0;36mencode_input\u001b[0;34m(data_dict)\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mquestion_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwtoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyLnEhhkeEWD"
      },
      "source": [
        "import numpy as np\n",
        "train_gen = tf.data.Dataset.from_generator(lambda: train_dataset,\n",
        "                                               output_signature=({'image':tf.TensorSpec(shape=(config.img_w, config.img_h, 3),dtype=np.uint8),\n",
        "                                                                 'question':tf.TensorSpec(shape=(config.max_length),dtype=np.int32)},\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "\n",
        "train_gen = train_gen.batch(config.batch_size)\n",
        "\n",
        "train_gen = train_gen.repeat()\n",
        "\n",
        "valid_gen = tf.data.Dataset.from_generator(lambda: val_dataset,\n",
        "                                              output_signature=({'image':tf.TensorSpec(shape=(config.img_w, config.img_h, 3),dtype=np.uint8),\n",
        "                                                                 'question':tf.TensorSpec(shape=(config.max_length),dtype=np.int32)},\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "valid_gen = valid_gen.batch(config.batch_size)\n",
        "\n",
        "valid_gen = valid_gen.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNKb_-pCAKRP"
      },
      "source": [
        "# Pre-Trained Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjTXrTCNB4mL",
        "outputId": "f9605ede-2d56-4d8e-b426-b502e3dec42c"
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 10:40:31--  https://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.840B.300d.zip [following]\n",
            "--2021-01-31 10:40:32--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768976 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  2.21MB/s    in 18m 38s \n",
            "\n",
            "2021-01-31 10:59:10 (1.86 MB/s) - ‘glove.840B.300d.zip’ saved [2176768976/2176768976]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6OCDfTzGStT",
        "outputId": "ef741aed-d5d0-49b7-a9a5-7222ab67e104"
      },
      "source": [
        "!unzip glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_1tt7MnANJo"
      },
      "source": [
        "%%capture\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "embeddings_index = dict()\n",
        "f = Path('./glove.840B.300d.txt').open()\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "    except Exception:\n",
        "      continue\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "\n",
        "size_of_vocabulary = config.wtoi+1\n",
        "embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
        "\n",
        "for word, i in config.tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVJDV1oXsaMx"
      },
      "source": [
        "#VQA Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kURrKKWWG_9K",
        "outputId": "d64c9b4d-ecc4-4af7-d283-3b1c0d5a5584"
      },
      "source": [
        "# Import Keras \n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , Input, LSTM, Embedding, Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "regularizer = tf.keras.regularizers.l2(1e-4)\n",
        "constraint = MaxNorm(3)\n",
        "# Define CNN for Image Input\n",
        "vision_model = Sequential()\n",
        "vision_model.add(Conv2D(64, (3, 3), padding='same', input_shape=(config.img_w, config.img_h, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(64, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(MaxPooling2D((2, 2)))\n",
        "vision_model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(128, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(MaxPooling2D((2, 2)))\n",
        "vision_model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(256, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(256, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "#vision_model.add(MaxPooling2D((2, 2)))\n",
        "#vision_model.add(Flatten())\n",
        "vision_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "image_input = Input(shape=(config.img_w, config.img_h, 3),name='image')\n",
        "encoded_image = vision_model(image_input)\n",
        "\n",
        "# Define RNN for language input\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, output_dim=256)(question_input)\n",
        "encoded_question = LSTM(256,kernel_constraint=constraint,recurrent_constraint=constraint)(embedded_question)\n",
        "\n",
        "# Combine CNN and RNN to create the final model\n",
        "merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n",
        "output = Dense(config.num_classes, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(merged)\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "question (InputLayer)           [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 14, 256)      608768      question[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "image (InputLayer)              [(None, 350, 200, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          525312      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 256)          1738944     image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           lstm_1[0][0]                     \n",
            "                                                                 sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 58)           29696       concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,902,720\n",
            "Trainable params: 2,900,416\n",
            "Non-trainable params: 2,304\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1mJoyg6toGn"
      },
      "source": [
        "# Visual Transfer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXTntBOtuLD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "67d25f01-4666-478c-acf8-8082d2792ff9"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , Input, LSTM, Embedding, Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "regularizer = tf.keras.regularizers.l2(1e-4)\n",
        "constraint = MaxNorm(3)\n",
        "# Define CNN for Image Input\n",
        "vision_model = tf.keras.applications.MobileNetV3Large(include_top=False, input_shape=(config.img_w, config.img_h, 3), pooling=\"avg\")\n",
        "for layer in vision_model.layers:\n",
        "  layer.trainable = False\n",
        "for i in range(-6, 0):\n",
        "  vision_model.layers[i].trainable = True\n",
        "image_input = Input(shape=(config.img_w, config.img_h, 3),name='image')\n",
        "encoded_image = vision_model(image_input)\n",
        "\n",
        "# Define RNN for language input\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, output_dim=256)(question_input)\n",
        "encoded_question = LSTM(256,kernel_constraint=constraint,recurrent_constraint=constraint)(embedded_question)\n",
        "\n",
        "# Combine CNN and RNN to create the final model\n",
        "merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n",
        "output = Dense(config.num_classes, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(merged)\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top.h5\n",
            "17612800/17605208 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-02a92bc3ba4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Define RNN for language input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mquestion_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0membedded_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwtoi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mencoded_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecurrent_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_size, name, dtype, sparse, tensor, ragged, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                      'to Input, not both at the same time.')\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbatch_input_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     raise ValueError('Please provide to Input either a `shape`'\n\u001b[0m\u001b[1;32m    297\u001b[0m                      \u001b[0;34m' or a `tensor` argument. Note that '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                      \u001b[0;34m'`shape` does not include the batch '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Please provide to Input either a `shape` or a `tensor` argument. Note that `shape` does not include the batch dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_cyC88HC1bD"
      },
      "source": [
        "# Visual and Text Transfer Learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xRXpko_C_bT",
        "outputId": "5d7a1211-49ba-456a-aa81-a6dc0232f30d"
      },
      "source": [
        "# Vision Model comes from: https://towardsdatascience.com/metastasis-detection-using-cnns-transfer-learning-and-data-augmentation-684761347b59\n",
        "from tensorflow.keras.layers import GRU, Dropout, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , GlobalMaxPooling2D, Input, LSTM, Embedding, Dense, Activation, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "regularizer = None\n",
        "constraint = None\n",
        "# Define CNN for Image Input\n",
        "vision_model = tf.keras.applications.MobileNetV3Large(include_top=False, input_shape=(config.img_w, config.img_h, 3))\n",
        "for layer in vision_model.layers:\n",
        "  layer.trainable = False\n",
        "#for i in range(-6, 0):\n",
        "#  vision_model.layers[i].trainable = True\n",
        "\n",
        "image_input = Input(shape=(config.img_w, config.img_h, 3),name='image')\n",
        "encoded_image = vision_model(image_input)\n",
        "pool_1 = GlobalAveragePooling2D()(encoded_image)\n",
        "pool_2 = GlobalMaxPooling2D()(encoded_image)\n",
        "#flatten = Flatten()(encoded_image)\n",
        "concat = tf.keras.layers.concatenate([pool_1, pool_2])\n",
        "encoded_image = Dropout(0.3)(concat)\n",
        "# Define RNN for language input\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, weights=[embedding_matrix], output_dim=300, trainable=False)(question_input)\n",
        "encoded_question = Bidirectional(GRU(256,\n",
        "                                     kernel_constraint=constraint,\n",
        "                                     recurrent_constraint=constraint,\n",
        "                                     bias_constraint=constraint, \n",
        "                                     kernel_regularizer=regularizer, \n",
        "                                     activity_regularizer=regularizer,\n",
        "                                     recurrent_regularizer=regularizer,\n",
        "                                     ))(embedded_question)\n",
        "\n",
        "# Combine CNN and RNN to create the final model\n",
        "merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n",
        "x = Dense(256, kernel_regularizer=regularizer, kernel_constraint=constraint)(merged)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "output = Dense(config.num_classes, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(x)\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 350, 200, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "MobilenetV3large (Functional)   (None, 11, 7, 1280)  4226432     image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "question (InputLayer)           [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_7 (Glo (None, 1280)         0           MobilenetV3large[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_7 (GlobalM (None, 1280)         0           MobilenetV3large[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 98560)        0           MobilenetV3large[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 14, 300)      710100      question[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 101120)       0           global_average_pooling2d_7[0][0] \n",
            "                                                                 global_max_pooling2d_7[0][0]     \n",
            "                                                                 flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 512)          857088      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 101120)       0           concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 101632)       0           bidirectional_5[0][0]            \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 16)           1626112     concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16)           64          dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16)           0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 256)          4096        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 256)          1024        dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 256)          0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 58)           14848       activation_11[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 7,439,764\n",
            "Trainable params: 2,502,688\n",
            "Non-trainable params: 4,937,076\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJA4IavpsvAN"
      },
      "source": [
        "#Questions Classification Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoR1DUgOs7fy"
      },
      "source": [
        "#yes_or_no is class 0, counting is class 1, other is class 2\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def create_class_questions_dict():\n",
        "  train_validation_dict = json.load(config.dataset_name.joinpath(\"train_questions_annotations.json\").open())\n",
        "  class_questions_dict = defaultdict(dict)\n",
        "  counter = 0\n",
        "  for v in train_validation_dict.values():\n",
        "    if v['answer'] in {'yes','no'}:\n",
        "      class_questions_dict[counter][\"question\"] = v['question']\n",
        "      class_questions_dict[counter][\"target\"] = 0\n",
        "    elif v['answer'] in {'0','1','2','3','4','5'}:\n",
        "      class_questions_dict[counter][\"question\"] = v['question']\n",
        "      class_questions_dict[counter][\"target\"] = 1\n",
        "    else:\n",
        "      class_questions_dict[counter][\"question\"] = v['question']\n",
        "      class_questions_dict[counter][\"target\"] = 2\n",
        "    counter += 1\n",
        "\n",
        "  return class_questions_dict\n",
        "\n",
        "#We need to split it and then to take from validation and training\n",
        "\n",
        "def split_class_questions(config : Config):\n",
        "  questions = create_class_questions_dict()\n",
        "  keys = list(questions.keys())\n",
        "  random.shuffle(keys)\n",
        "  training_samples = int(len(keys) * config.split)\n",
        "  training_keys = keys[0:training_samples]\n",
        "  validation_keys = keys[training_samples:]\n",
        "  training_dict = {}\n",
        "  validation_dict = {}\n",
        "  for k in training_keys:\n",
        "    training_dict[k] = questions[k]\n",
        "\n",
        "  for k in validation_keys:\n",
        "    validation_dict[k] = questions[k]\n",
        "  \n",
        "  return training_dict,validation_dict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihn1slJjs3-H"
      },
      "source": [
        "class CustomDatasetQuestions(tf.keras.utils.Sequence):\n",
        "  def __init__(self,config : Config,which_subset : str):\n",
        "    self.config = config\n",
        "    training_dict, validation_dict = split_class_questions(config)\n",
        "    self.data_dict = training_dict if which_subset == 'training' else validation_dict\n",
        "    self.encoder_inputs = encode_input(self.data_dict)\n",
        "    print(list(self.data_dict.items())[0:20])\n",
        "  def __len__(self):\n",
        "    return len(self.data_dict)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item_key = list(self.data_dict.keys())[index]\n",
        "    target = self.data_dict[item_key][\"target\"]\n",
        "    return self.encoder_inputs[index] , target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzk9YQu4tCMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c41b29-4a4f-4dd1-888e-c1dc8ce71a05"
      },
      "source": [
        "train_dataset = CustomDatasetQuestions(config,\"training\")\n",
        "val_dataset = CustomDatasetQuestions(config,\"validation\")\n",
        "\n",
        "import numpy as np\n",
        "train_gen = tf.data.Dataset.from_generator(lambda: train_dataset,\n",
        "                                               output_signature=(\n",
        "                                                                 tf.TensorSpec(shape=(config.max_length),dtype=np.int32),\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "\n",
        "train_gen = train_gen.batch(config.batch_size)\n",
        "\n",
        "train_gen = train_gen.repeat()\n",
        "\n",
        "valid_gen = tf.data.Dataset.from_generator(lambda: val_dataset,\n",
        "                                               output_signature=(\n",
        "                                                                 tf.TensorSpec(shape=(config.max_length),dtype=np.int32),\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "\n",
        "valid_gen = valid_gen.batch(config.batch_size)\n",
        "\n",
        "valid_gen = valid_gen.repeat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(24453, {'question': 'Is the man scared?', 'target': 0}), (56254, {'question': 'What kind of ball is this?', 'target': 2}), (8605, {'question': 'Are all the plants the same height?', 'target': 0}), (13510, {'question': 'Is it sunny?', 'target': 0}), (27204, {'question': 'Is it possible that the man is in danger?', 'target': 0}), (50584, {'question': 'What color is the couch?', 'target': 2}), (39959, {'question': 'What is the weather outside?', 'target': 2}), (19790, {'question': 'Is the woman feeling lonely?', 'target': 0}), (17273, {'question': 'Is the girl falling?', 'target': 0}), (26327, {'question': 'What colors are in the area rug?', 'target': 2}), (36199, {'question': 'What is the old man holding?', 'target': 2}), (57715, {'question': 'What is next to the dog?', 'target': 2}), (24570, {'question': 'Is the woman alone?', 'target': 0}), (51807, {'question': 'Is there a person on the table top?', 'target': 0}), (48578, {'question': 'Are all of the individuals featured in this picture male?', 'target': 0}), (53861, {'question': 'What is the bicycle standing against?', 'target': 2}), (30406, {'question': 'Is the old lady looking at the cat?', 'target': 0}), (52964, {'question': \"Does the man's clothing match the couch?\", 'target': 0}), (4105, {'question': \"What color is the baby's shirt?\", 'target': 2}), (29085, {'question': 'What type of fruit is on the table?', 'target': 2})]\n",
            "[(52947, {'question': 'What is this girl thinking?', 'target': 2}), (32441, {'question': 'How many pillows on the couch?', 'target': 1}), (58656, {'question': 'Does the boy wearing socks?', 'target': 0}), (17102, {'question': 'How many children are in the park?', 'target': 1}), (43375, {'question': 'Is there someone hanging on the monkey bars?', 'target': 0}), (57755, {'question': 'Are both ladies sitting?', 'target': 0}), (39339, {'question': 'What color is his shirt?', 'target': 2}), (22722, {'question': 'What is the person holding?', 'target': 2}), (31595, {'question': 'Is the boy taking the bread from the owl?', 'target': 0}), (27253, {'question': 'How many plants are there?', 'target': 1}), (38272, {'question': 'Does the man look scared?', 'target': 0}), (28873, {'question': \"Is the dog's tail reminiscent of the cloud?\", 'target': 0}), (49228, {'question': 'How many pots on top of the fireplace?', 'target': 1}), (1543, {'question': 'How many hamburgers are on the blanket?', 'target': 1}), (1289, {'question': 'What color is the wine?', 'target': 2}), (56082, {'question': 'Are there curtains on the window?', 'target': 0}), (56259, {'question': \"What is the girl's hand?\", 'target': 2}), (31576, {'question': 'Does a child live in this house?', 'target': 0}), (3124, {'question': 'Where is she taking the food?', 'target': 2}), (43098, {'question': 'How many birds are afraid of the dog?', 'target': 1})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyitCZGrtEgT"
      },
      "source": [
        "#Question Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo6KCVhntIX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41beda16-3b24-4944-b26a-f8506e926a00"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , Input, LSTM, Embedding, Dense, Activation, BatchNormalization, Bidirectional,GRU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "constraint = MaxNorm(3)\n",
        "regularizer = tf.keras.regularizers.l1_l2(1e-2)\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, output_dim=256)(question_input)\n",
        "encoded_question = Bidirectional(GRU(16,kernel_constraint=constraint,recurrent_constraint=constraint, bias_constraint=constraint, bias_regularizer=regularizer, recurrent_regularizer=regularizer, kernel_regularizer=regularizer, activity_regularizer=regularizer))(embedded_question)\n",
        "output = Dense(3, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(encoded_question)\n",
        "model = Model(inputs=question_input, outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "question (InputLayer)        [(None, 14)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, 14, 256)           627456    \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 32)                26304     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 96        \n",
            "=================================================================\n",
            "Total params: 653,856\n",
            "Trainable params: 653,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBNqH10csgGW"
      },
      "source": [
        "#Compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG5n9rsQixFc"
      },
      "source": [
        "from focal_loss import SparseCategoricalFocalLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIPGWnQ3J0dQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "d3a391bf-c020-4007-f976-7000df2cc1e7"
      },
      "source": [
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "loss = SparseCategoricalFocalLoss(gamma=2)\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "\n",
        "metrics = ['sparse_categorical_accuracy']\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e6bb3540cc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Compile Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH0iJMnssjCp"
      },
      "source": [
        "#Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fmfDGuDK6qy"
      },
      "source": [
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
        "import os \n",
        "\n",
        "exps_dir = Path(\"/\").joinpath(\"content\", \"drive\", \"MyDrive\", \"Colab Notebooks\", \"Homework3\",\"Results\")\n",
        "exps_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'NaiveModel_' + config.questions\n",
        "\n",
        "exp_dir = Path(exps_dir).joinpath(model_name + '_' + str(now))\n",
        "exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "ckpt_dir = Path(exp_dir).joinpath('ckpts')\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                   save_weights_only=True)  # False to save the model directly\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "# ---------------------------------\n",
        "tb_dir = Path(exp_dir).joinpath('tb_logs')\n",
        "tb_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=0)  # if 1 shows weights histograms\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    callbacks.append(es_callback)\n",
        "\n",
        "# Learning Rate Annhealing\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', patience=3, verbose=1, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "lr_scheduling = True\n",
        "if lr_scheduling:\n",
        "    callbacks.append(learning_rate_reduction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl8uFqu1sk99"
      },
      "source": [
        "#Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-sj_H5oSKdXz",
        "outputId": "d11967ea-ecf3-47fc-eb9a-ca9ca09b5a33"
      },
      "source": [
        "\n",
        "steps_train = len(train_dataset) // config.batch_size \n",
        "steps_val = len(val_dataset) // config.batch_size \n",
        "model.fit(x=train_gen,\n",
        "          epochs=100,  #### set repeat in training dataset\n",
        "          steps_per_epoch=steps_train,\n",
        "          validation_data=valid_gen,\n",
        "          validation_steps=steps_val, \n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "735/735 [==============================] - 62s 80ms/step - loss: 5.5282 - sparse_categorical_accuracy: 0.5407 - val_loss: 0.4008 - val_sparse_categorical_accuracy: 0.6595\n",
            "Epoch 2/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.3847 - sparse_categorical_accuracy: 0.6874 - val_loss: 0.3532 - val_sparse_categorical_accuracy: 0.7062\n",
            "Epoch 3/100\n",
            "735/735 [==============================] - 57s 78ms/step - loss: 0.3521 - sparse_categorical_accuracy: 0.7023 - val_loss: 0.3414 - val_sparse_categorical_accuracy: 0.7069\n",
            "Epoch 4/100\n",
            "735/735 [==============================] - 57s 78ms/step - loss: 0.3375 - sparse_categorical_accuracy: 0.7037 - val_loss: 0.3213 - val_sparse_categorical_accuracy: 0.7078\n",
            "Epoch 5/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.3195 - sparse_categorical_accuracy: 0.7043 - val_loss: 0.3111 - val_sparse_categorical_accuracy: 0.7079\n",
            "Epoch 6/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.3105 - sparse_categorical_accuracy: 0.7050 - val_loss: 0.3060 - val_sparse_categorical_accuracy: 0.7079\n",
            "Epoch 7/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.3056 - sparse_categorical_accuracy: 0.7056 - val_loss: 0.3027 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 8/100\n",
            "735/735 [==============================] - 58s 78ms/step - loss: 0.3025 - sparse_categorical_accuracy: 0.7049 - val_loss: 0.2997 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 9/100\n",
            "735/735 [==============================] - 58s 78ms/step - loss: 0.2999 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.2976 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 10/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2980 - sparse_categorical_accuracy: 0.7048 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 11/100\n",
            "735/735 [==============================] - 58s 78ms/step - loss: 0.2801 - sparse_categorical_accuracy: 0.7044 - val_loss: 0.2769 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 12/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2779 - sparse_categorical_accuracy: 0.7048 - val_loss: 0.2768 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 13/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2765 - sparse_categorical_accuracy: 0.7051 - val_loss: 0.2758 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 14/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2690 - sparse_categorical_accuracy: 0.7058 - val_loss: 0.2670 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 15/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2681 - sparse_categorical_accuracy: 0.7057 - val_loss: 0.2676 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 16/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2680 - sparse_categorical_accuracy: 0.7044 - val_loss: 0.2665 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 17/100\n",
            "735/735 [==============================] - 57s 78ms/step - loss: 0.2639 - sparse_categorical_accuracy: 0.7050 - val_loss: 0.2627 - val_sparse_categorical_accuracy: 0.7079\n",
            "Epoch 18/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2632 - sparse_categorical_accuracy: 0.7062 - val_loss: 0.2628 - val_sparse_categorical_accuracy: 0.7079\n",
            "Epoch 19/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2626 - sparse_categorical_accuracy: 0.7070 - val_loss: 0.2625 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 20/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2609 - sparse_categorical_accuracy: 0.7063 - val_loss: 0.2605 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 21/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2606 - sparse_categorical_accuracy: 0.7064 - val_loss: 0.2605 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 22/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2603 - sparse_categorical_accuracy: 0.7068 - val_loss: 0.2603 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 23/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2595 - sparse_categorical_accuracy: 0.7060 - val_loss: 0.2594 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 24/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2590 - sparse_categorical_accuracy: 0.7071 - val_loss: 0.2594 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 25/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2594 - sparse_categorical_accuracy: 0.7060 - val_loss: 0.2593 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "Epoch 26/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2589 - sparse_categorical_accuracy: 0.7056 - val_loss: 0.2588 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 27/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2592 - sparse_categorical_accuracy: 0.7047 - val_loss: 0.2588 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 28/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2590 - sparse_categorical_accuracy: 0.7050 - val_loss: 0.2587 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "Epoch 29/100\n",
            "735/735 [==============================] - 57s 78ms/step - loss: 0.2588 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.2585 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 30/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2584 - sparse_categorical_accuracy: 0.7067 - val_loss: 0.2585 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 31/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2581 - sparse_categorical_accuracy: 0.7076 - val_loss: 0.2585 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "Epoch 32/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.7076 - val_loss: 0.2583 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 33/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.2583 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 34/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.7072 - val_loss: 0.2583 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "Epoch 35/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2577 - sparse_categorical_accuracy: 0.7079 - val_loss: 0.2583 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 36/100\n",
            "735/735 [==============================] - 58s 79ms/step - loss: 0.2577 - sparse_categorical_accuracy: 0.7080 - val_loss: 0.2583 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 37/100\n",
            "735/735 [==============================] - 60s 81ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.7080 - val_loss: 0.2583 - val_sparse_categorical_accuracy: 0.7080\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "Epoch 38/100\n",
            "735/735 [==============================] - 60s 82ms/step - loss: 0.2574 - sparse_categorical_accuracy: 0.7086 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 39/100\n",
            "735/735 [==============================] - 60s 82ms/step - loss: 0.2574 - sparse_categorical_accuracy: 0.7084 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 40/100\n",
            "735/735 [==============================] - 61s 82ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.7078 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 41/100\n",
            "735/735 [==============================] - 60s 82ms/step - loss: 0.2578 - sparse_categorical_accuracy: 0.7070 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 42/100\n",
            "735/735 [==============================] - 62s 85ms/step - loss: 0.2577 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 43/100\n",
            "735/735 [==============================] - 61s 83ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.7078 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 44/100\n",
            "735/735 [==============================] - 60s 82ms/step - loss: 0.2577 - sparse_categorical_accuracy: 0.7072 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 45/100\n",
            "735/735 [==============================] - 59s 80ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 46/100\n",
            "735/735 [==============================] - 60s 81ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.7064 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 47/100\n",
            "735/735 [==============================] - 60s 82ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.7065 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 48/100\n",
            "735/735 [==============================] - 60s 81ms/step - loss: 0.2579 - sparse_categorical_accuracy: 0.7066 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 49/100\n",
            "735/735 [==============================] - 61s 83ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 50/100\n",
            "735/735 [==============================] - 60s 82ms/step - loss: 0.2577 - sparse_categorical_accuracy: 0.7072 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 51/100\n",
            "735/735 [==============================] - 60s 81ms/step - loss: 0.2577 - sparse_categorical_accuracy: 0.7075 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 52/100\n",
            "735/735 [==============================] - 61s 83ms/step - loss: 0.2574 - sparse_categorical_accuracy: 0.7085 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 53/100\n",
            "735/735 [==============================] - 62s 85ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.7079 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 54/100\n",
            "735/735 [==============================] - 66s 90ms/step - loss: 0.2576 - sparse_categorical_accuracy: 0.7080 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 55/100\n",
            "735/735 [==============================] - 65s 88ms/step - loss: 0.2575 - sparse_categorical_accuracy: 0.7082 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 56/100\n",
            "735/735 [==============================] - 64s 87ms/step - loss: 0.2572 - sparse_categorical_accuracy: 0.7092 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 57/100\n",
            "735/735 [==============================] - 64s 88ms/step - loss: 0.2573 - sparse_categorical_accuracy: 0.7086 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 58/100\n",
            "735/735 [==============================] - 64s 87ms/step - loss: 0.2575 - sparse_categorical_accuracy: 0.7079 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 59/100\n",
            "735/735 [==============================] - 65s 88ms/step - loss: 0.2573 - sparse_categorical_accuracy: 0.7084 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 60/100\n",
            "735/735 [==============================] - 64s 87ms/step - loss: 0.2573 - sparse_categorical_accuracy: 0.7083 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 61/100\n",
            "735/735 [==============================] - 64s 88ms/step - loss: 0.2575 - sparse_categorical_accuracy: 0.7079 - val_loss: 0.2582 - val_sparse_categorical_accuracy: 0.7080\n",
            "Epoch 62/100\n",
            "660/735 [=========================>....] - ETA: 6s - loss: 0.2571 - sparse_categorical_accuracy: 0.7089"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-2753982f4f2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqvXMqbqsouR"
      },
      "source": [
        "#Model prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1mL3TgcuybW"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')\n",
        "\n",
        "def prediction(model):\n",
        "  test_questions = json.load(config.dataset_name.joinpath(\"test_questions.json\").open())\n",
        "  questions = encode_input(test_questions)\n",
        "  counter = 0\n",
        "  results = {}\n",
        "  for k,v in test_questions.items():\n",
        "    image = Image.open(Path(config.dataset_name).joinpath(\"Images\",v[\"image_id\"] + \".png\"))\n",
        "    item_image = image.convert(\"RGB\")\n",
        "    item_image = item_image.resize((config.img_w,config.img_h))\n",
        "    item_image_arr = np.array(item_image).transpose((1, 0, 2))\n",
        "    item_image_arr = np.float32(item_image_arr) / 255.0\n",
        "    item_image_arr = tf.expand_dims(item_image_arr,axis=0)\n",
        "    input_dict = {'image':item_image_arr , 'question':tf.expand_dims(questions[counter],axis=0)}\n",
        "    results[k] = model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    counter +=1\n",
        "  create_csv(results)\n",
        "\n",
        "def prediction_ensemble(config,yes_or_no_model,counting_model,other_model,classifier_model):\n",
        "  test_questions = json.load(config.dataset_name.joinpath(\"test_questions.json\").open())\n",
        "  questions = encode_input(test_questions)\n",
        "  counter = 0\n",
        "  results = {}\n",
        "  for k,v in test_questions.items():\n",
        "    image = Image.open(Path(config.dataset_name).joinpath(\"Images\",v[\"image_id\"]+\".png\"))\n",
        "    item_image = image.convert(\"RGB\")\n",
        "    item_image = item_image.resize((config.img_w,config.img_h))\n",
        "    item_image_arr = np.array(item_image).transpose((1, 0, 2))\n",
        "    item_image_arr = np.float32(item_image_arr) / 255.0\n",
        "    question_type = classifier_model.predict(questions[counter]).argmax(axis=-1)[0]\n",
        "    input_dict = {'image':tf.expand_dims(item_image_arr,axis=0) , 'question':tf.expand_dims(questions[counter],axis=0)}\n",
        "    if question_type == 0:\n",
        "      results[k] = yes_or_no_model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    elif question_type == 1:\n",
        "      results[k] = counting_model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    else:\n",
        "      results[k] = other_model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    counter +=1\n",
        "  create_csv(results)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bCDXUfaiavy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e60998-d81c-47fe-dedc-5fbcf98199af"
      },
      "source": [
        "prediction_ensemble(config,yes_or_no_model,model_counting,model_other,question_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 14) for input KerasTensor(type_spec=TensorSpec(shape=(None, 14), dtype=tf.int32, name='question'), name='question', description=\"created by layer 'question'\"), but it was called on an input with incompatible shape (None, 1).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}